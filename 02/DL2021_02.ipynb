{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "DL2021_02.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFx4YvLOXzIz"
      },
      "source": [
        "# 521153S:3, Deep Learning assignment 2: Loss Function, Optimization, Neural Networks, Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpCpwYnXzI8"
      },
      "source": [
        "## Outline \n",
        "#### In this assignment, you will learn:\n",
        "* **Neural Network, Deep Neural Network, Loss Function and Optimization**.\n",
        "* Building a one layer NN using numpy to understand the backpropagation.\n",
        "* Gradient check using finite-difference approximation.\n",
        "* Stochatic Gradient Descent (SGD).\n",
        "* Simple hype-parameters tuning methods to improve your NN performance.\n",
        "\n",
        "#### Grading (<span style=\"color:green\">15 points</span>)\n",
        "In this assignment, we are going to learn about **Neural Network, Deep Neural Network, Loss Function and Optimization**. They are very important knowledge you need to know in deep learning.\n",
        "\n",
        "Hints: First of all , make sure you implement the sigmoid, softmax and cross-entropy loss **correctly**. <br>\n",
        "Then, **implement first the weight decay off (is_weight_decay = False) then move on with (is_weight_decay = True)**\n",
        "* **Part 1.** Import libraries, loading and preprocessing the training and testing data.\n",
        "* **Part 2.** Optimize the neural network according to the loss function. (<span style=\"color:green\">12 points</span>)\n",
        "  * 2.1. Construct model in **Fig. 2** (<span style=\"color:green\">4 points</span>) <br>\n",
        "       * 2.1.1 Declare W1 and W2 (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.1.2 Implement sigmoid (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.1.3 Implement softmax (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.1.4 Implement the cross-entropy loss (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "  * 2.2. Implement the forward-pass (<span style=\"color:green\">2.0 point</span>) <br>\n",
        "       * 2.2.1 Foward-pass in training (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.2.2 Call cross entropy loss in training (<span style=\"color:green\">0.5 point</span>) <br>\n",
        "       * 2.2.3 Forward-pass in testing (<span style=\"color:green\">0.5 point</span>) <br>\n",
        "  * 2.3. Implement SGD (<span style=\"color:green\">2 point</span>) <br>\n",
        "       * 2.3.1 Velocity (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.3.2 Update weights (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "  * 2.4. Derivatives <br>\n",
        "       * Derive equations <br> \n",
        "  * 2.5. Implement the derivatives (<span style=\"color:green\">4.0 points</span>) <br>\n",
        "       * 2.5.1 Implement equation 8 (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.5.2 Implement equation 6 (<span style=\"color:green\">0.5 point</span>) <br>\n",
        "       * 2.5.3 Implement equation 10 (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.5.4 Implement equation 10+11 (<span style=\"color:green\">1.0 point</span>) <br>\n",
        "       * 2.5.5 Implement equation 7 (<span style=\"color:green\">0.5 point</span>) <br>\n",
        "* **Part 3.** Gradient check using finite-difference approximation. (<span style=\"color:green\">0.5 points</span>) (YOU **DON'T NEED** TO WRITE THE CODE FOR THIS) <br>\n",
        "  * Question: why don't we use FDA to calculate the gradient to update our model? (<span style=\"color:green\">0.5 point</span>) <br>\n",
        "* **Part 4.** Regularization and NN simple tunning. (<span style=\"color:green\">2.5 points</span>) <br>\n",
        "  * 4.1. Applying weight decay. (<span style=\"color:green\">1.5 point</span>) <br>\n",
        "       * There are 3 spots you need to fill, (<span style=\"color:green\">0.5 point</span>) each spot.\n",
        "  * 4.2. Change the number of neurons in the hidden layer and report the performance (<span style=\"color:green\">1 point</span>) <br>\n",
        "\n",
        "#### Environment\n",
        "Python 3, Numpy, matplotlib, sklearn\n",
        "\n",
        "#### Dataset\n",
        "* [**Fashion-MNIST**](https://github.com/zalandoresearch/fashion-mnist)\n",
        "is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. Using the Fashion-MNIST give you more room to wiggle your experiments.\n",
        "\n",
        "#### Hints\n",
        "* To find the place where you have to insert your solution, hit Crtl + F and search for **TODO:** . You are NOT suppose to modify the codes from other parts.\n",
        "* **Be careful with the shape** of the weights, gradient, .. of your tensor in your implementation. Double check and make sure the shapes are fit for computation, especially matrix multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqUZblntXzJA"
      },
      "source": [
        "## Part 1. Import libraries, loading and preprocessing the training and testing data\n",
        "**You don't need to change the code from this part.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD0vjfgKXzJC"
      },
      "source": [
        "# You will mainly use numpy to construct your NN\n",
        "import numpy as np\n",
        "import matplotlib, time, copy, os, requests, zipfile, sys\n",
        "# Matplotlib to plot the image\n",
        "import matplotlib.pyplot as plt\n",
        "# Off-the-shelf evaluation functions provided by sklearn\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "# Matplotlib predefined 'magic function'. It will include your graphs in your notebook, next to the code\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SplTCcjeXzJF"
      },
      "source": [
        "### Functions use to download the dataset from google drive\n",
        "The code snipet was taken from [this thread](https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi9g---sXzJH"
      },
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH2-YqAdXzJI"
      },
      "source": [
        "### Functions use to pre-process your training/testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLFgwBtEXzJJ"
      },
      "source": [
        "def reshape_train_data(X):\n",
        "    ''' Input training data has shape (60000, 28, 28)\n",
        "        Input testing data has shape (10000, 28, 28)\n",
        "        where: \n",
        "        60000 is the numbers of input training samples\n",
        "        10000 is the numbers of input testing samples\n",
        "        similar to MNIST, resolution of each sample is 28 x 28\n",
        "    '''\n",
        "    samples, H, W = X.shape\n",
        "    # Reshape input volume to (sample, 784), this mean, your NN input layer will have 784 placeholders\n",
        "    # we scale the RGB values by divide them by 255, this will help improve the training performance\n",
        "    return X.reshape(samples, H * W).T / 255\n",
        "\n",
        "def one_hot_vector(x, num_classes):\n",
        "    # By now, I think you already heard about this so many times\n",
        "    return np.eye(num_classes)[x].T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK85G5EcXzJK"
      },
      "source": [
        "### We took care of download the data for you.\n",
        "The fashion-MNIST data will be download and store in your **work_dir/data/fashion_mnist_npy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGji_weYXzJM"
      },
      "source": [
        "PATH = './data'\n",
        "if not os.path.exists(PATH):\n",
        "    os.makedirs(PATH)\n",
        "    \n",
        "    file_id = '1DQ2Nf2rY467kyZKOf_CG3Kib5FLv0xQu'\n",
        "    destination = os.path.join(PATH, 'fashion_mnist_npy.zip')\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "    \n",
        "    with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
        "        zip_ref.extractall(PATH)\n",
        "        \n",
        "    print(\"Data downloaded and extracted!\")\n",
        "    \n",
        "    os.remove(destination)\n",
        "    \n",
        "else:\n",
        "    print(\"Data was already downloaded and extracted!\")\n",
        "\n",
        "PATH = os.path.join(PATH, 'fashion_mnist_npy')\n",
        "\n",
        "# The actual meaning of the label of your classes.\n",
        "# E.g. if a output one-hot vector is [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], it used to prepresent a Dress\n",
        "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYoDWlzfXzJN"
      },
      "source": [
        "### Finally, we load and preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ADRe-_riXzJP"
      },
      "source": [
        "# Load the training input\n",
        "X_train = np.load(os.path.join(PATH, 'train_data.npy'))\n",
        "# Load the training labels\n",
        "X_test = np.load(os.path.join(PATH, 'test_data.npy'))\n",
        "# Load the testing input\n",
        "Y_train = np.load(os.path.join(PATH, 'train_labels.npy'))\n",
        "# Load the testing labels\n",
        "Y_test = np.load(os.path.join(PATH, 'test_labels.npy'))\n",
        "\n",
        "# Get the number of classes\n",
        "num_classes = len(label_names)\n",
        "# Get the number of training samples and their resolution for reshape\n",
        "num_trains, HEIGHT, WIDTH = X_train.shape\n",
        "\n",
        "# Reshape the training and testing inputs\n",
        "X_train, X_test = reshape_train_data(X_train), reshape_train_data(X_test)\n",
        "\n",
        "# Create one-hot vector for the training and testing labels\n",
        "Y_train, Y_test = one_hot_vector(Y_train, num_classes), one_hot_vector(Y_test, num_classes)\n",
        "\n",
        "\n",
        "# This part use to randomly load some of the training and testing image and the one-hot vectors for checking\n",
        "fig_train, ax_train = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
        "fig_train.suptitle(\"Random image from the TRAINING set\", y=0.73, fontsize=16, fontweight='bold')\n",
        "\n",
        "fig_test, ax_test = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
        "fig_test.suptitle(\"Random image from the TESTING set\", y=0.73, fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx in range(5):\n",
        "    i, j = np.random.randint(num_trains), np.random.randint(X_test.shape[0])\n",
        "    \n",
        "    ax_train[idx].imshow(X_train[:,i].reshape(HEIGHT, WIDTH), cmap = matplotlib.cm.binary)\n",
        "    ax_train[idx].set_title(label_names[np.argmax(Y_train[:,i])] + \"\\n\" + str(Y_train[:,i]))\n",
        "    ax_train[idx].axis('off')\n",
        "    \n",
        "    ax_test[idx].imshow(X_test[:,j].reshape(HEIGHT, WIDTH), cmap = matplotlib.cm.binary)\n",
        "    ax_test[idx].set_title(label_names[np.argmax(Y_test[:,j])] + \"\\n\" + str(Y_test[:,j]))\n",
        "    ax_test[idx].axis('off')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eapiVYjGXzJQ"
      },
      "source": [
        "## Part 2. Loss function and optimization (<span style=\"color:green\">10 points</span>)\n",
        "#### The fun part start from here.\n",
        "*Definition:* loss function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. Loss function (sometime called cost function) is used to calculate the gradient by applying the chain rule, so that we can update the weights (and bias) of our neural network using optimizer such as gradient descent. <br>\n",
        "It could be interpreted as using loss function to calculate the network’s layers error, in which we calculate the error of the current layer, then pass the weighted error back to the previous layer, and recursively doing this until we travel back to the first hidden layer. At each layer, we update the weights using the derivative of the cost for each weight.\n",
        "\n",
        "Let illustrate this concept using a simple toy example.\n",
        "\n",
        "![toy.png](https://github.com/tvaisanen/deep-learning-2021/blob/main/02/figs/toy.png?raw=1)\n",
        "<center> <strong> <font size=\"3\" color=\"blue\"> Figure 1. Toy example of backpropagation </font> </strong> </center>\n",
        "\n",
        "In **Fig. 1** we have a loss function $L$:\n",
        "\\begin{align}\n",
        "L = c \\times d\n",
        "\\end{align}\n",
        "\n",
        "where <br>\n",
        "\n",
        "\\begin{align}\n",
        "c = a + b -5\n",
        "\\end{align}\n",
        "\n",
        "and, <br>\n",
        "\n",
        "\\begin{align}\n",
        "d = b^2 + b -1\n",
        "\\end{align}\n",
        "\n",
        "As illustrated in **Fig. 1**, the equations in the edge show the partial derivation of some functions with respect to their direct variables. For example, $\\frac{\\partial L}{\\partial c}$ is the partial derivative of $L$ with respect to $c$. <br>\n",
        "However, we are actually interested in calculate the derivative of $L$ with respect to $a$ and $b$, which don't directly connected with each other. So, how can we do this? As mentioned earlier, we can do this using the chain-rule so that we can calculate $\\frac{\\partial L}{\\partial a}$ and $\\frac{\\partial L}{\\partial b}$ as:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial c} \\cdot \\frac{\\partial c}{\\partial a}\n",
        "\\end{align}\n",
        "\n",
        "and, <br>\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} + \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial b}\n",
        "\\end{align}\n",
        "\n",
        "More importantly, **we can see that $a$ affect $L$ through $c$, and so on**. This concept hold no matter how many hidden layers you have or how complicated your loss will be as long as you use backpropagation to calculate the derivative to update your weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI_jWtVaXzJQ"
      },
      "source": [
        "Now, let's get your hand dirty. Let's take a look at the model in **Fig. 2** below. This is the model we are gonna use in this assignment.\n",
        "\n",
        "![NN.png](https://github.com/tvaisanen/deep-learning-2021/blob/main/02/figs/NN.png?raw=1)\n",
        "<center> <strong> <font size=\"4\" color=\"blue\"> Figure 2. Structure of our neural network </font> </strong> </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsJfhoyuXzJR"
      },
      "source": [
        "### Construct model base on **Fig. 2** above (<span style=\"color:green\">4 points</span>)\n",
        "As shown in the **Fig. 2**, our neural network contains two hidden layer (we use 7 and 3 neurons as the  first hidden layer and the second hidden layer default setting respectively) and an output layer. To further simply things, we will not include biases in our model. Be aware that, while the number of neuron in the hidden layer can be abitrary, there must be 10 neurons in the output layer because we want our model to classify images from 10 classes. Based on the **Fig. 2**, you have to construct your network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THvhHN98XzJR"
      },
      "source": [
        "# Numbers of input units\n",
        "num_input = X_train.shape[0]\n",
        "# Number of neural in your hidden layer\n",
        "# TODO: modify the number of neurons in the hidden layer\n",
        "num_hidden_1 = 7\n",
        "num_hidden_2 = 3\n",
        "\n",
        "# Construct your neural network from Fig. 2 (1.0 point)\n",
        "# TODO: Random initialize the hidden_1 layer weights (w/o bias)\n",
        "# Hints: use np.random.randn()\n",
        "W1 = \n",
        "# TODO: Random initialize the hidden_2 layer weights (w/o bias)\n",
        "# Hints: use np.random.randn()\n",
        "W2 = \n",
        "\n",
        "# TODO: Random initialize the output layer weights (w/o bias)\n",
        "# Hints: use np.random.randn()\n",
        "W3 = \n",
        "\n",
        "def sigmoid(X):\n",
        "    # TODO: implement Eq. 2 (1.0 point)\n",
        "    # Hints: use np.exp()\n",
        "    \n",
        "\n",
        "def softmax(X):\n",
        "    # TODO: implement Eq. 4 (1.0 point)\n",
        "    # Hints: use np.exp() and np.sum(, axis=0) <- beware of the axis\n",
        "    \n",
        "\n",
        "def cross_entropy_loss(Y, Y_pred):\n",
        "    # TODO: implement Eq. 5 (1.0 point)\n",
        "    # Hints: use np.sum(), np.multiply() and np.log()\n",
        "    # At the end, we need to divide by the number of of sample in the training batch e.g. M = Y.shape[1]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCEwKNvCXzJS"
      },
      "source": [
        "According to the model in **Fig. 2**, our **forward-propagation** is going to start with the calculation of the output volume $A_1$ for the first hidden layer:\n",
        "\n",
        "\\begin{align}\n",
        "A_1 = \\sigma(Z_1) = \\sigma(W_1 X ) \\tag{1}\n",
        "\\end{align}\n",
        "where $W_1$ is the weights of the first hidden layer, $X$ is the input, and $\\sigma$ is the sigmoid activation where: <br>\n",
        "\\begin{align}\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}} \\tag{2}\n",
        "\\end{align}\n",
        "\n",
        "Then, we calculate the hidden volume of the second hidden layer $A_2$ by: <br>\n",
        "\\begin{align}\n",
        "A_2 = \\sigma(Z_2) = \\sigma(W_2 A_1 ) \\tag{3}\n",
        "\\end{align}\n",
        "where $W_2$ is the weights of the second hidden layer.\n",
        "\n",
        "Finally, we calculate the output volume of the output layer $A_3$ by: <br>\n",
        "\\begin{align}\n",
        "A_3 = S(Z_3) = S(W_3 A_2) \\tag{4}\n",
        "\\end{align}\n",
        "\n",
        "In **Eq. 4**, $W_3$ is the weights of the output layer, $A_2$ is the output volume of the last hidden layer, $S$ stands for softmax and defines by: <br>\n",
        "\\begin{align}\n",
        "S(x_i) = \\frac{e^{x_i}}{\\sum_{j=0}^{k} e^{x_j} } \\tag{5}\n",
        "\\end{align}\n",
        "where $i=0,1,..,k$. We use $k$ to represent classes, and $k=9$ in our case.\n",
        "\n",
        "Finally, we compute the loss function using:\n",
        "\n",
        "\\begin{align}\n",
        "L(Y, A_3) = - \\frac{1}{M} \\sum_{k=0}^{M} \\sum_{i=0}^{N} Y_i^{k} log({A_3}_i^{k}) \\tag{6}\n",
        "\\end{align}\n",
        "\n",
        "where $Y$ is the ground truth labels, $N$ is the number of classes, $M$ is the number of samples in the training batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5dCcY8XzJT"
      },
      "source": [
        "### Derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcLhN57YXzJU"
      },
      "source": [
        "Using what you learnt in undergraduate school, let's calculate the **backward-propagation** for our model in **Fig. 2**. <br>\n",
        "As mentioned earlier, we are interested in $\\frac{\\partial L}{\\partial W_1}$, $\\frac{\\partial L}{\\partial W_2}$ and $\\frac{\\partial L}{\\partial W_3}$, where: \n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial W_3} = \\frac{\\partial L}{\\partial A_3} \\cdot \\frac{\\partial A_3}{\\partial Z_3} \\cdot \\frac{\\partial Z_3}{\\partial W_3} \\tag{7}\n",
        "\\end{align}\n",
        "\n",
        ",<br>\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial A_3} \\cdot \\frac{\\partial A_3}{\\partial Z_3} \\cdot \\frac{\\partial Z_3}{\\partial A_2} \\cdot \\frac{\\partial A_2}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial W_2} \\tag{8}\n",
        "\\end{align}\n",
        "\n",
        "and, <br> \n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial A_3} \\cdot \\frac{\\partial A_3}{\\partial Z_3} \\cdot \\frac{\\partial Z_3}{\\partial A_2} \\cdot \\frac{\\partial A_2}{\\partial Z_2} \\cdot \\frac{\\partial Z_2} {\\partial A_1} \\cdot \\frac{\\partial A_1}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W_1} \\tag{9}\n",
        "\\end{align}\n",
        " \n",
        "From **Eq. 6** we have: <br>\n",
        "\n",
        "The derivative of $L$ with respect to $Z_3$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial Z_3} = \\frac{\\partial L}{\\partial A_3} \\cdot \\frac{\\partial A_3}{\\partial Z_3} = A_3 - Y \\tag{10}\n",
        "\\end{align}\n",
        "\n",
        "The derivative of $Z_3$ with respect to $W_3$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial Z_3}{\\partial W_3} = A_2 \\tag{11}\n",
        "\\end{align}\n",
        "\n",
        "From **Eq. 7** we have: <br>\n",
        "\n",
        "The derivative of $L$ with respect to $A_2$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial A_2} = \\frac{\\partial L}{\\partial Z_3} \\cdot \\frac{\\partial Z_3}{\\partial A_2} = (A_3 - Y) W_3\\tag{12}\n",
        "\\end{align}\n",
        "\n",
        "The derivative of $A_2$ with respect to $Z_2$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial A_2}{\\partial Z_2} = (A_2) (1 - A_2) \\tag{13}\n",
        "\\end{align}\n",
        "\n",
        "The derivative of $Z_2$ with respect to $W_2$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial Z_2}{\\partial W_2} = A_1 \\tag{14}\n",
        "\\end{align}\n",
        "\n",
        "From **Eq. 8** we have: <br>\n",
        "\n",
        "The derivative of $L$ with respect to $A_1$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial A_2} \\cdot \\frac{\\partial A_2}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial A_1} = (A_3 - Y) W_3 (A_2) (1 - A_2) W_2 \\tag{15}\n",
        "\\end{align}\n",
        "\n",
        "The derivative of $A_1$ with respect to $Z_1$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial A_1}{\\partial Z_1} = (A_1) (1 - A_1) \\tag{16}\n",
        "\\end{align}\n",
        "\n",
        "The derivative of $Z_1$ with respect to $W_1$: \n",
        "\\begin{align}\n",
        "\\frac{\\partial Z_1}{\\partial W_1} = X \\tag{17}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GhsLEJiXzJV"
      },
      "source": [
        "## Part. 3 Gradient check using finite-difference approximation. (<span style=\"color:green\">0.5 points</span>)\n",
        "\n",
        "When training deep neural network, there are many things that can go wrong. Until this point, you probably notice that we have been going on and on about the gradient. Why? Because the gradient is very important. Hence, you must making sure that the calculation of your gradient is correct. A correct gradient calculation won't promise your model will converge, but if the calculation was wrong your model will perform very weird. This type of error is hard to debug, so we better prevent it beforehand. <br> \n",
        "To perform the gradient check, we can calculate gradient using the [finite-difference approximation](https://en.wikipedia.org/wiki/Finite_difference) (FDA), and let's call the output of FDA numerical gradients. Then we compare this numerical gradients with the gradient we calculate from taking the derivative. If the differences between them are small enough, we can assume that the gradient was calculated correctly. <br>\n",
        "\n",
        "You probably learnt about FDA in your undergraduate, but to refresh your mind, let's have a simple example to see how FDA works. Assume that we have a function $f(x)$ which <br>\n",
        "\n",
        "\\begin{align}\n",
        "f(x) = \\frac{1}{3} x^3 - \\frac{1}{2} x^2 + 1 \\tag{18}\n",
        "\\end{align}\n",
        "\n",
        "Then, the derivative $\\Delta f$ will be: <br>\n",
        "\\begin{align}\n",
        "\\Delta f = x^2 - x \\tag{19}\n",
        "\\end{align}\n",
        "\n",
        "At $x=2.125$, using **Eq. 19** we have $\\Delta f = 2.390625$\n",
        "\n",
        "If we calculate the numerical gradient using FDA we have:\n",
        "\\begin{align}\n",
        "\\Delta_{num\\_grad} f = \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2 * \\epsilon}  \\tag{20}\n",
        "\\end{align}\n",
        "\n",
        "where $\\epsilon$ is a very small value (E.g. $\\epsilon = 1e-{04}$)\n",
        "\n",
        "At the same point $x=2.125$, using **Eq. 15** we have $\\Delta_{num\\_grad} f = 2.3906250033389753$ <br>\n",
        "\n",
        "We can see that, the calculated values of $\\Delta f$ and $\\Delta_{num\\_grad} f$ are very close to each other. <br>\n",
        "\n",
        "With the same idea, we can check the gradient calculation of our network using FDA. A simple way to do this is: <br>\n",
        "1) We wiggle (by a very small $\\epsilon$ values) the value of our weight for all of the parameters in our model. By all parameters, I mean all of the weights of $W_1$, $W_2$ and $W_3$. E.g. if we use 7 neurons in the first hidden layer and 3 neurons in the second hidden layer, the number of parameters in our network is : <br> \n",
        "$num\\_params(net) = num\\_params(W_1) + num\\_params(W_2) + num\\_params(W_3) = 28*28*7 + 7*7*3 + 3*10 = 5665$ <br>\n",
        "so we have to repeat the \"wiggling\" and calculate the numerical gradient 5665 times. At the end, we have a $num\\_grad$ vector that have shape (5665,) <br>\n",
        "2) Calculate the gradient by taking the derivative. Similarly, we will have a $grad$ vector that also have shape (5665,) <br>\n",
        "3) Compare $num\\_grad$ and $grad$ vectors by: <br>\n",
        "\\begin{align}\n",
        "grad\\_diff = \\frac{||grad - num\\_grad||_2}{|grad + num\\_grad|_2}  \\tag{21}\n",
        "\\end{align}\n",
        "\n",
        "If **grad_diff** is smaller than $1e-{08}$ than we assume that our gradient calculation is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg77TS92XzJV"
      },
      "source": [
        "### Question: Why don't we use FDA to calculate the gradient to update our model? (<span style=\"color:green\">0.5 points</span>) <br>\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezXe8H4nXzJW"
      },
      "source": [
        "**Answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Xl2yxuXzJW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVFB2GtIXzJX"
      },
      "source": [
        "### Hyper-parameters in your training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRuzhoSzXzJX"
      },
      "source": [
        "# Flag use to enable/disable weight decay regularization\n",
        "is_weight_decay = False\n",
        "\n",
        "if is_weight_decay:\n",
        "    # Setting lambda coefficient for weight decay\n",
        "    lmda = np.exp(-7)\n",
        "\n",
        "# Seting learning rate and momentum for SGD\n",
        "learning_rate = 0.25\n",
        "beta = 0.5\n",
        "\n",
        "# Seting the number of training epochs\n",
        "epoch = 50\n",
        "# Choose your batch size\n",
        "batch_size = 128\n",
        "# Calculate the number of training iterations base on the number of training samples and your batch size\n",
        "num_batchs = num_trains // batch_size\n",
        "\n",
        "print(\"Num_trains: {}, num_batchs: {}\".format(num_trains, num_batchs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO-hQ1KDXzJY"
      },
      "source": [
        "### Training your network (<span style=\"color:green\">9.5 points</span>)\n",
        "\n",
        "In this assignment, we will train our model using mini-batch stochatic gradient descent with momentum. To know more about this optimization algorithm, please check out this great [video](https://www.youtube.com/watch?v=k8fTYJPd3_I) from Dr. Andrew Ng. <br>\n",
        "For this assignment, we will use the implementation from **Eq. 22** and **Eq. 23** <br>\n",
        "\n",
        "\\begin{align}\n",
        "v_{dW} = \\beta v_{dW} + (1 - \\beta) dW \\tag{22}\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "W = W - \\alpha v_{dW} \\tag{23}\n",
        "\\end{align}\n",
        "\n",
        "If you are curious, you can modify the hyper-params in the above section at your will."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dQrErY-DXzJY"
      },
      "source": [
        "# Define an interactive ipython figure to display the training loss\n",
        "%matplotlib notebook\n",
        "fig = plt.figure(\"Training loss\")\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "plt.ion()\n",
        "fig.show()\n",
        "fig.canvas.draw()\n",
        "\n",
        "# Log the training loss\n",
        "loss_log = []\n",
        "\n",
        "# Zeros initialize the momentum for SGD\n",
        "V_dW1 = np.zeros(W1.shape)\n",
        "V_dW2 = np.zeros(W2.shape)\n",
        "V_dW3 = np.zeros(W3.shape)\n",
        "\n",
        "# Training\n",
        "for i in range(epoch):\n",
        "    start_t = time.time()\n",
        "    \n",
        "    # Random shuffle training data every training epoch\n",
        "    np.random.seed(np.random.randint(num_trains))\n",
        "    indices = np.random.permutation(num_trains)\n",
        "    X_train_shuffled, Y_train_shuffled = X_train[:, indices], Y_train[:, indices]\n",
        "\n",
        "    for j in range(num_batchs):\n",
        "\n",
        "        # Get mini-batch samples for training\n",
        "        start_idx = j * batch_size\n",
        "        end_idx = min(j * batch_size + batch_size, X_train.shape[1] - 1)\n",
        "        X, Y = X_train_shuffled[:, start_idx : end_idx], Y_train_shuffled[:, start_idx : end_idx]\n",
        "        # Size of actual mini-batch, it could be smaller than batch_size\n",
        "        mini_batch = end_idx - start_idx\n",
        "        \n",
        "        # TODO: implement the forward-pass (1.0 point)\n",
        "        Z1 = \n",
        "        A1 = \n",
        "        Z2 = \n",
        "        A2 = \n",
        "        Z3 = \n",
        "        A3 =\n",
        "        \n",
        "\n",
        "        if is_weight_decay: \n",
        "            # TODO: call cross entropy loss with weight decay regularization\n",
        "            # We need to penalize both W1, W2 and W3 here with the lmda coefficient defined above\n",
        "            # (0.5 point)\n",
        "            L =\n",
        "            \n",
        "        else:\n",
        "            # TODO: call cross entropy loss (0.5 point)\n",
        "            L =\n",
        "            \n",
        "        \n",
        "        # Log the training loss during training\n",
        "        loss_log.append(L)\n",
        "        \n",
        "\n",
        "        # TODO: calculate the derivative of 𝐿 with respect to 𝑍3 using eq. 10 (1.0 point)\n",
        "        dZ3 = \n",
        "        \n",
        "        if is_weight_decay:\n",
        "            # TODO: calculate the derivative of 𝐿 with respect to W3 using eq. 7 with weight decay regularization\n",
        "            # We only need penalize and W3 here with the lmda coefficient defined above\n",
        "            # (0.5 point)\n",
        "            dW3 =\n",
        "            \n",
        "        else:\n",
        "            # TODO: calculate the derivative of 𝐿 with respect to W2 using eq. 7 (0.5 point)\n",
        "            # Hints: consider the mini_batch during the calculation of the gradient\n",
        "            # use np.matmul() and transpose the matrix to fit the dimension\n",
        "            dW3 =\n",
        "        \n",
        "        \n",
        "        # TODO: calculate the derivative of 𝐿 with respect to A2 using eq. 12 (1.0 point)\n",
        "        # Hints: use np.matmul() and transpose the matrix to fit the dimension\n",
        "        dA2 = \n",
        "        \n",
        "        # TODO: calculate the derivative of 𝐿 with respect to Z2 using eq. 12 and eq. 13 (1.0 point)\n",
        "        # Hints: use element-wise multiplication * \n",
        "        dZ2 = \n",
        "        \n",
        "        if is_weight_decay:\n",
        "            # TODO: calculate the derivative of 𝐿 with respect to W2 using eq. 7 with weight decay regularization\n",
        "            # We only need penalize and W1 here with the lmda coefficient defined above\n",
        "            # (0.5 point)\n",
        "            dW2 =\n",
        "            \n",
        "        else:\n",
        "            # TODO: calculate the derivative of 𝐿 with respect to W2 using eq. 8 (0.5 point)\n",
        "            # Hints: similarly, consider the mini_batch during the calculation of the gradient\n",
        "            # use np.matmul() and transpose the matrix to fit the dimension\n",
        "            dW2 =            \n",
        "            \n",
        "        # TODO: calculate the derivative of 𝐿 with respect to A1 using eq. 15 (1.0 point)\n",
        "        # Hints: use np.matmul() and transpose the matrix to fit the dimension\n",
        "        dA1 = \n",
        "        \n",
        "        # TODO: calculate the derivative of 𝐿 with respect to Z1 using eq. 15 and eq. 16 (1.0 point)\n",
        "        # Hints: use element-wise multiplication * \n",
        "        dZ1 = \n",
        "        \n",
        "        if is_weight_decay:\n",
        "            # TODO: calculate the derivative of 𝐿 with respect to W1 using eq. 9 with weight decay regularization\n",
        "            # We only need penalize and W1 here with the lmda coefficient defined above\n",
        "            # (0.5 point)\n",
        "            dW1 =\n",
        "            \n",
        "        else:\n",
        "            # TODO: calculate the derivative of 𝐿 with respect to W1 using eq. 9 (0.5 point)\n",
        "            # Hints: similarly, consider the mini_batch during the calculation of the gradient\n",
        "            # use np.matmul() and transpose the matrix to fit the dimension\n",
        "            dW1 =         \n",
        "        \n",
        "        # TODO: Update the learning velocity using Eq. 22 (1.0 point)\n",
        "        V_dW1 = \n",
        "        V_dW2 = \n",
        "        V_dW3 = \n",
        "\n",
        "        # TODO: Update the model weights using Eq. 23 (1.0 point)\n",
        "        W1 = \n",
        "        W2 = \n",
        "        W3 = \n",
        "\n",
        "        if (j % 100 == 0):\n",
        "            print(\"[Epoch/Iterations]:[{}/{}], loss: {}\".format(i, j, L))\n",
        "            \n",
        "    ax.clear()\n",
        "    ax.plot(loss_log)\n",
        "    fig.canvas.draw()\n",
        "    print(\"=> Elapsed time epoch #{} : {:.2f} seconds\".format(i, time.time() - start_t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILxeyp2lXzJa"
      },
      "source": [
        "### Evaluate the performance of your model (<span style=\"color:green\">0.5 points</span>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcDIMPYfXzJa"
      },
      "source": [
        "# TODO: implement the forward-pass (0.5 point)\n",
        "# Hints: note that this is similar but not exactly the same as the forward pass during training\n",
        "Z1 = \n",
        "A1 = \n",
        "Z2 = \n",
        "A2 = \n",
        "Z3 = \n",
        "A3 = \n",
        "\n",
        "# Evaluate the performance of your NN\n",
        "predictions = np.argmax(A3, axis=0)\n",
        "labels = np.argmax(Y_test, axis=0)\n",
        "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(labels, predictions)))\n",
        "print(\"Testing accuracy: {}\".format(accuracy_score(labels, predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTenPZB4XzJb"
      },
      "source": [
        "## Part 4. Regularization and NN simple tunning (<span style=\"color:green\">2.5 points</span>)\n",
        "\n",
        "1. Applying weight decay (<span style=\"color:green\">1.5 point</span> )\n",
        "  * Using what you learnt from assignment 1 to add the code at neccesary parts in **Training your network** above.\n",
        "  * There are 3 spots, (<span style=\"color:green\">0.5 point</span>) each spot. Insert the answers in above section.\n",
        "2. Change the number of neurons in the hidden layer and report the performance (<span style=\"color:green\">1 point</span>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el3csSDWXzJb"
      },
      "source": [
        "**Answer of question 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LNhjk7hXzJb"
      },
      "source": [
        ""
      ]
    }
  ]
}